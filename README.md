# Spark Optimization Mini-Project

### Description:
Spark can give you a tremendous advantage when it comes to quickly processing massive datasets. However, the tool is only as powerful as the one who wields it. Spark performance can become sluggish if poor decisions are made in the layout of the code and functions that are chosen.

The exercise gives hands-on experience optimizing PySpark code. I will look at the physical plan for the query execution and then modify the query to improve performance.

We want to compose a query such that we get for each question also the number of answers to this question for each month. A version exists inside ".py" file. My task is to rewrite and achieve more optimal performance.

### Ways to Improve performance of Spark job:
1. By picking the right operators.
2. Reduce the number of shuffles and the amount of data shuffled
3. Tuning Resource Allocation
4. Tuning the Number of Partitions
5. Reducing the Size of Data Structures
6. Choosing Data Formats

